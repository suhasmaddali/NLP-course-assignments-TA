{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "caf6dbb2",
   "metadata": {},
   "source": [
    "## CS 6120: Natural Language Processing - Prof. Ahmad Uzair\n",
    "\n",
    "### Assignment 1: Naive Bayes\n",
    "### Total Points: 100\n",
    "\n",
    "In this assignment, you would be working with SMS data that contains SPAM or HAM messages. When you take a look at your gmail account, you find that a few mails are classified as spam. Similarly, some text messages that are received on the phone are also classified as spam based on a set of characteristics such as wording and so on. \n",
    "\n",
    "Therefore, we are going to address this problem of detecting SPAM or HAM messages with the help of Naive Bayes algorithm.\n",
    "\n",
    "## Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "06e9ca3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731a6c8b",
   "metadata": {},
   "source": [
    "## Reading the data\n",
    "\n",
    "When reading the data, ensure that the '.csv' file is in the same location where your jupyter notebook is used. This way the files are organized and easy to read using the pandas library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c709e89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('spam.csv', sep = ',', encoding = 'latin-1', usecols = lambda col: col not in [\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0792fda4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "674a0a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns = {\"v1\": \"spam_or_ham\", \"v2\": \"message\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e061ac93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spam_or_ham</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  spam_or_ham                                            message\n",
       "0         ham  Go until jurong point, crazy.. Available only ...\n",
       "1         ham                      Ok lar... Joking wif u oni...\n",
       "2        spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3         ham  U dun say so early hor... U c already then say...\n",
       "4         ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "66f5d6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "38524991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(df['spam_or_ham'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adde1a42",
   "metadata": {},
   "source": [
    "## Upsampling the minority class: (5 points)\n",
    "\n",
    "It is known that Naive bayes is not robust to class imbalance. It could be seen above that the data is quite imbalanced. Therefore, class balancing must be done before giving it to the Naive Bayes model for prediction. \n",
    "\n",
    "Feel free to use 'resample' library from sklearn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "92c50a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "## hint: use resample from sklearn.utils\n",
    "from sklearn.utils import resample\n",
    "\n",
    "df_majority = df[df.spam_or_ham == 'ham']\n",
    "df_minority = df[df.spam_or_ham == 'spam']\n",
    "\n",
    "spam_upsample = resample(df_minority, replace = True, \n",
    "                        n_samples = df_majority.shape[0],\n",
    "                        random_state = 101)\n",
    "\n",
    "df_upsampled = pd.concat([df_majority, spam_upsample])\n",
    "df_upsampled = df_upsampled.sample(frac = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6f64d480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spam_or_ham</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3022</th>\n",
       "      <td>ham</td>\n",
       "      <td>You are a very very very very bad girl. Or lady.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3270</th>\n",
       "      <td>spam</td>\n",
       "      <td>You have 1 new voicemail. Please call 08719181...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4194</th>\n",
       "      <td>spam</td>\n",
       "      <td>Double mins and txts 4 6months FREE Bluetooth ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2841</th>\n",
       "      <td>ham</td>\n",
       "      <td>aathi..where are you dear..</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5066</th>\n",
       "      <td>spam</td>\n",
       "      <td>83039 62735=å£450 UK Break AccommodationVouche...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     spam_or_ham                                            message\n",
       "3022         ham   You are a very very very very bad girl. Or lady.\n",
       "3270        spam  You have 1 new voicemail. Please call 08719181...\n",
       "4194        spam  Double mins and txts 4 6months FREE Bluetooth ...\n",
       "2841         ham                        aathi..where are you dear..\n",
       "5066        spam  83039 62735=å£450 UK Break AccommodationVouche..."
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_upsampled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ec6786d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK CELL\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    '''\n",
    "    Input:\n",
    "        tweet: a string containing a tweet\n",
    "    Output:\n",
    "        tweets_clean: a list of words containing the processed tweet\n",
    "\n",
    "    '''\n",
    "    stemmer = PorterStemmer()\n",
    "    english_stopwords = stopwords.words('english')\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    punc = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "    for text in tweet:\n",
    "        if text in punc:\n",
    "            tweet = tweet.replace(text, \"\")\n",
    "    tweet_tokens = word_tokenize(tweet)\n",
    "\n",
    "    tweets_cleaned = []\n",
    "    for word in tweet_tokens:\n",
    "        if (word not in english_stopwords):  # remove stopwords\n",
    "            # tweets_clean.append(word)\n",
    "            stem_word = stemmer.stem(word)  # stemming word\n",
    "            tweets_cleaned.append(stem_word)\n",
    "\n",
    "    return tweets_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7414d5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK CELL\n",
    "def find(frequency, word, label):\n",
    "    '''\n",
    "    Params:\n",
    "        frequency: a dictionary with the frequency of each pair (or tuple)\n",
    "        word: the word to look up\n",
    "        label: the label corresponding to the word\n",
    "    Return:\n",
    "        n: the number of times the word with its corresponding label appears.\n",
    "    '''\n",
    "    n = 0  \n",
    "\n",
    "    pair = (word, label)\n",
    "    if (pair in frequency):\n",
    "        n = frequency[pair]\n",
    "\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "248be64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6720c80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['message']\n",
    "y = df['spam_or_ham']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d1365873",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_map = {'ham': 0, 'spam': 1}\n",
    "y = y.map(output_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5ca0c6ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       0\n",
       "2       1\n",
       "3       0\n",
       "4       0\n",
       "       ..\n",
       "5567    1\n",
       "5568    0\n",
       "5569    0\n",
       "5570    0\n",
       "5571    0\n",
       "Name: spam_or_ham, Length: 5572, dtype: int64"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d8defa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "96595501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No did you check? I got his detailed message now'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "32140cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['no', 'check', 'i', 'got', 'detail', 'messag']\n"
     ]
    }
   ],
   "source": [
    "custom_tweet = X_train.iloc[0]\n",
    "\n",
    "# print cleaned tweet\n",
    "print(clean_tweet(custom_tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1f52ed76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK CELL\n",
    "def tweet_counter(output, tweets, tweet_senti):\n",
    "    '''\n",
    "    Params:\n",
    "        output: a dictionary that will be used to map each pair to its frequency\n",
    "        tweets: a list of tweets\n",
    "        tweet_senti: a list corresponding to the sentiment of each tweet (either 0 or 1)\n",
    "    Return:\n",
    "        output: a dictionary mapping each pair to its frequency\n",
    "    '''\n",
    "\n",
    "\n",
    "    for label, tweet in zip(tweet_senti, tweets):\n",
    "        for word in clean_tweet(tweet):\n",
    "            # define the key, which is the word and label tuple\n",
    "            tweet_and_label = (word,label)\n",
    "\n",
    "            # if the key exists in the dictionary, increment the count\n",
    "            if tweet_and_label in output:\n",
    "                output[tweet_and_label] += 1\n",
    "\n",
    "            # else, if the key is new, add it to the dictionary and set the count to 1\n",
    "            else:\n",
    "                output[tweet_and_label] = 1\n",
    "\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "cfd6f6e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('get', 1): 2,\n",
       " ('offer', 1): 1,\n",
       " ('upto', 1): 1,\n",
       " ('20', 1): 1,\n",
       " ('i', 0): 1,\n",
       " ('come', 0): 1,\n",
       " ('click', 1): 1,\n",
       " ('link', 1): 1,\n",
       " ('latest', 1): 1,\n",
       " ('car', 1): 1,\n",
       " ('canva', 0): 1,\n",
       " ('class', 0): 1,\n",
       " ('schedul', 0): 1}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing your function\n",
    "\n",
    "\n",
    "result = {}\n",
    "tweets = ['get offer upto 20%', 'I am coming now', 'Click on the link', 'get a latest car', 'canvas class scheduled']\n",
    "ys = [1, 0, 1, 1, 0]\n",
    "tweet_counter(result, tweets, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "78070398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the freqs dictionary for later uses\n",
    "\n",
    "freqs = tweet_counter({}, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "132ffee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('no', 0): 116,\n",
       " ('check', 0): 38,\n",
       " ('i', 0): 1160,\n",
       " ('got', 0): 191,\n",
       " ('detail', 0): 18,\n",
       " ('messag', 0): 53,\n",
       " ('for', 0): 18,\n",
       " ('love', 0): 183,\n",
       " ('start', 0): 49,\n",
       " ('attractioni', 0): 1,\n",
       " ('feel', 0): 72,\n",
       " ('need', 0): 148,\n",
       " ('everi', 0): 35,\n",
       " ('time', 0): 167,\n",
       " ('around', 0): 51,\n",
       " ('mesh', 0): 1,\n",
       " ('first', 0): 45,\n",
       " ('thing', 0): 94,\n",
       " ('come', 0): 231,\n",
       " ('thoughtsi', 0): 1,\n",
       " ('would', 0): 63,\n",
       " ('day', 0): 164,\n",
       " ('end', 0): 35,\n",
       " ('hersh', 0): 1,\n",
       " ('dreamlov', 0): 1,\n",
       " ('breath', 0): 3,\n",
       " ('namemi', 0): 1,\n",
       " ('life', 0): 59,\n",
       " ('happen', 0): 40,\n",
       " ('hermi', 0): 1,\n",
       " ('name', 0): 22,\n",
       " ('heri', 0): 2,\n",
       " ('cri', 0): 5,\n",
       " ('herwil', 0): 1,\n",
       " ('give', 0): 81,\n",
       " ('happi', 0): 95,\n",
       " ('take', 0): 114,\n",
       " ('sorrowsi', 0): 1,\n",
       " ('readi', 0): 29,\n",
       " ('fight', 0): 11,\n",
       " ('anyon', 0): 14,\n",
       " ('craziest', 0): 2,\n",
       " ('herlov', 0): 2,\n",
       " ('dont', 0): 212,\n",
       " ('proov', 0): 1,\n",
       " ('girl', 0): 31,\n",
       " ('beauti', 0): 19,\n",
       " ('ladi', 0): 7,\n",
       " ('whole', 0): 12,\n",
       " ('planeti', 0): 1,\n",
       " ('alway', 0): 43,\n",
       " ('sing', 0): 5,\n",
       " ('prais', 0): 1,\n",
       " ('make', 0): 111,\n",
       " ('chicken', 0): 4,\n",
       " ('curri', 0): 1,\n",
       " ('maki', 0): 1,\n",
       " ('sambarlif', 0): 1,\n",
       " ('thenwil', 0): 1,\n",
       " ('get', 0): 279,\n",
       " ('morn', 0): 67,\n",
       " ('thank', 0): 67,\n",
       " ('god', 0): 29,\n",
       " ('mei', 0): 5,\n",
       " ('like', 0): 196,\n",
       " ('say', 0): 119,\n",
       " ('lotwil', 0): 2,\n",
       " ('tell', 0): 104,\n",
       " ('later', 0): 107,\n",
       " ('think', 0): 127,\n",
       " ('spatula', 0): 1,\n",
       " ('hand', 0): 21,\n",
       " ('great', 0): 79,\n",
       " ('run', 0): 25,\n",
       " ('ttyl', 0): 6,\n",
       " ('eastend', 1): 2,\n",
       " ('tv', 1): 5,\n",
       " ('quiz', 1): 9,\n",
       " ('what', 1): 5,\n",
       " ('flower', 1): 3,\n",
       " ('dot', 1): 2,\n",
       " ('compar', 1): 2,\n",
       " ('d=', 1): 2,\n",
       " ('violet', 1): 2,\n",
       " ('e=', 1): 2,\n",
       " ('tulip', 1): 2,\n",
       " ('f=', 1): 2,\n",
       " ('lili', 1): 2,\n",
       " ('txt', 1): 136,\n",
       " ('d', 1): 2,\n",
       " ('e', 1): 4,\n",
       " ('f', 1): 4,\n",
       " ('84025', 1): 3,\n",
       " ('now', 1): 51,\n",
       " ('4', 1): 90,\n",
       " ('chanc', 1): 24,\n",
       " ('2', 1): 151,\n",
       " ('win', 1): 49,\n",
       " ('å£100', 1): 18,\n",
       " ('cash', 1): 50,\n",
       " ('wkent150p16+', 1): 2,\n",
       " ('bruce', 0): 2,\n",
       " ('b', 0): 47,\n",
       " ('down', 0): 3,\n",
       " ('amp', 0): 65,\n",
       " ('fletcher', 0): 1,\n",
       " ('2', 0): 251,\n",
       " ('doc', 0): 7,\n",
       " ('appoint', 0): 4,\n",
       " ('next', 0): 42,\n",
       " ('week', 0): 60,\n",
       " ('im', 0): 365,\n",
       " ('tire', 0): 12,\n",
       " ('shove', 0): 3,\n",
       " ('stuff', 0): 32,\n",
       " ('ugh', 0): 11,\n",
       " ('couldnt', 0): 5,\n",
       " ('normal', 0): 7,\n",
       " ('bodi', 0): 4,\n",
       " ('babe', 0): 58,\n",
       " ('answer', 0): 20,\n",
       " ('cant', 0): 98,\n",
       " ('see', 0): 116,\n",
       " ('mayb', 0): 31,\n",
       " ('youd', 0): 8,\n",
       " ('better', 0): 28,\n",
       " ('reboot', 0): 3,\n",
       " ('ym', 0): 4,\n",
       " ('photo', 0): 8,\n",
       " ('it', 0): 148,\n",
       " ('do', 0): 92,\n",
       " ('u', 0): 776,\n",
       " ('konw', 0): 2,\n",
       " ('waht', 0): 2,\n",
       " ('rael', 0): 2,\n",
       " ('friendship', 0): 19,\n",
       " ('gving', 0): 2,\n",
       " ('yuo', 0): 4,\n",
       " ('exmpel', 0): 2,\n",
       " ('jsut', 0): 2,\n",
       " ('ese', 0): 3,\n",
       " ('tih', 0): 4,\n",
       " ('msg', 0): 46,\n",
       " ('evrey', 0): 2,\n",
       " ('splle', 0): 2,\n",
       " ('wrnog', 0): 2,\n",
       " ('bt', 0): 17,\n",
       " ('sitll', 0): 2,\n",
       " ('ra', 0): 2,\n",
       " ('wihtuot', 0): 2,\n",
       " ('ayn', 0): 2,\n",
       " ('mitsak', 0): 2,\n",
       " ('goodnight', 0): 7,\n",
       " ('have', 0): 74,\n",
       " ('a', 0): 72,\n",
       " ('nice', 0): 50,\n",
       " ('sleepsweet', 0): 2,\n",
       " ('dream', 0): 20,\n",
       " ('ju', 0): 27,\n",
       " ('finish', 0): 56,\n",
       " ('avatar', 0): 3,\n",
       " ('nigro', 0): 1,\n",
       " ('are', 0): 69,\n",
       " ('freez', 0): 2,\n",
       " ('home', 0): 124,\n",
       " ('yet', 0): 39,\n",
       " ('will', 0): 41,\n",
       " ('rememb', 0): 26,\n",
       " ('kiss', 0): 28,\n",
       " ('mom', 0): 21,\n",
       " ('miss', 0): 98,\n",
       " ('hi', 0): 92,\n",
       " ('babi', 0): 25,\n",
       " ('cruisin', 0): 1,\n",
       " ('friend', 0): 73,\n",
       " ('r', 0): 108,\n",
       " ('call', 0): 226,\n",
       " ('hour', 0): 37,\n",
       " ('that', 0): 122,\n",
       " ('alright', 0): 16,\n",
       " ('fone', 0): 9,\n",
       " ('jenni', 0): 2,\n",
       " ('xxx', 0): 18,\n",
       " ('bear', 1): 2,\n",
       " ('pic', 1): 20,\n",
       " ('nick', 1): 1,\n",
       " ('tom', 1): 1,\n",
       " ('pete', 1): 1,\n",
       " ('dick', 1): 1,\n",
       " ('in', 1): 3,\n",
       " ('fact', 1): 1,\n",
       " ('type', 1): 1,\n",
       " ('tri', 1): 31,\n",
       " ('gay', 1): 5,\n",
       " ('chat', 1): 34,\n",
       " ('photo', 1): 3,\n",
       " ('upload', 1): 2,\n",
       " ('call', 1): 299,\n",
       " ('08718730666', 1): 2,\n",
       " ('10pmin', 1): 9,\n",
       " ('stop', 1): 90,\n",
       " ('text', 1): 107,\n",
       " ('08712460324', 1): 5,\n",
       " ('okay', 0): 22,\n",
       " ('good', 0): 175,\n",
       " ('problem', 0): 29,\n",
       " ('thanx', 0): 27,\n",
       " ('wa', 0): 28,\n",
       " ('ur', 0): 199,\n",
       " ('openin', 0): 1,\n",
       " ('sentenc', 0): 3,\n",
       " ('formal', 0): 1,\n",
       " ('anyway', 0): 25,\n",
       " ('fine', 0): 38,\n",
       " ('juz', 0): 17,\n",
       " ('tt', 0): 4,\n",
       " ('eatin', 0): 7,\n",
       " ('much', 0): 87,\n",
       " ('n', 0): 115,\n",
       " ('puttin', 0): 2,\n",
       " ('weighthaha', 0): 1,\n",
       " ('so', 0): 101,\n",
       " ('anythin', 0): 3,\n",
       " ('special', 0): 28,\n",
       " ('sad', 0): 19,\n",
       " ('stori', 0): 17,\n",
       " ('man', 0): 37,\n",
       " ('last', 0): 53,\n",
       " ('bday', 0): 12,\n",
       " ('my', 0): 104,\n",
       " ('wife', 0): 16,\n",
       " ('didnt', 0): 60,\n",
       " ('wish', 0): 59,\n",
       " ('parent', 0): 14,\n",
       " ('forgot', 0): 25,\n",
       " ('kid', 0): 14,\n",
       " ('went', 0): 50,\n",
       " ('work', 0): 104,\n",
       " ('even', 0): 70,\n",
       " ('colleagu', 0): 9,\n",
       " ('know', 0): 203,\n",
       " ('rape', 0): 1,\n",
       " ('dude', 0): 20,\n",
       " ('poker', 0): 2,\n",
       " ('xma', 0): 12,\n",
       " ('radio', 0): 3,\n",
       " ('if', 0): 97,\n",
       " ('how', 0): 147,\n",
       " ('queen', 0): 4,\n",
       " ('go', 0): 344,\n",
       " ('royal', 0): 2,\n",
       " ('wed', 0): 10,\n",
       " ('sent', 0): 44,\n",
       " ('de', 0): 21,\n",
       " ('webadr', 0): 1,\n",
       " ('gete', 0): 1,\n",
       " ('salari', 0): 4,\n",
       " ('slip', 0): 2,\n",
       " ('ive', 0): 65,\n",
       " ('bar', 0): 5,\n",
       " ('q', 0): 2,\n",
       " ('store', 0): 4,\n",
       " ('lifethi', 0): 1,\n",
       " ('twat', 0): 1,\n",
       " ('orang', 0): 3,\n",
       " ('dungere', 0): 1,\n",
       " ('came', 0): 21,\n",
       " ('ask', 0): 101,\n",
       " ('want', 0): 180,\n",
       " ('deck', 0): 1,\n",
       " ('punch', 0): 1,\n",
       " ('whi', 0): 22,\n",
       " ('lunch', 0): 36,\n",
       " ('is', 0): 60,\n",
       " ('telli', 0): 2,\n",
       " ('brdget', 0): 1,\n",
       " ('jone', 0): 1,\n",
       " ('wake', 0): 22,\n",
       " ('long', 0): 37,\n",
       " ('ago', 0): 9,\n",
       " ('alreadi', 0): 76,\n",
       " ('dunno', 0): 24,\n",
       " ('aaooooright', 0): 1,\n",
       " ('just', 0): 76,\n",
       " ('look', 0): 48,\n",
       " ('addi', 0): 2,\n",
       " ('goe', 0): 15,\n",
       " ('back', 0): 108,\n",
       " ('monday', 0): 8,\n",
       " ('suck', 0): 7,\n",
       " ('did', 0): 49,\n",
       " ('receiv', 0): 8,\n",
       " ('hey', 0): 83,\n",
       " ('chief', 0): 1,\n",
       " ('bell', 0): 2,\n",
       " ('talk', 0): 48,\n",
       " ('visit', 0): 9,\n",
       " ('1st', 0): 8,\n",
       " ('june', 0): 3,\n",
       " ('you', 0): 169,\n",
       " ('ok', 0): 212,\n",
       " ('said', 0): 71,\n",
       " ('then', 0): 41,\n",
       " ('ì', 0): 94,\n",
       " ('dad', 0): 28,\n",
       " ('pick', 0): 67,\n",
       " ('lar', 0): 31,\n",
       " ('ìï', 0): 45,\n",
       " ('wan', 0): 67,\n",
       " ('stay', 0): 25,\n",
       " ('6', 0): 22,\n",
       " ('meh', 0): 9,\n",
       " ('still', 0): 115,\n",
       " ('area', 0): 3,\n",
       " ('restaur', 0): 2,\n",
       " ('ill', 0): 195,\n",
       " ('tri', 0): 64,\n",
       " ('soon', 0): 43,\n",
       " ('two', 0): 26,\n",
       " ('fundament', 0): 1,\n",
       " ('cool', 0): 32,\n",
       " ('walk', 0): 23,\n",
       " ('either', 0): 10,\n",
       " ('raglan', 0): 1,\n",
       " ('rd', 0): 5,\n",
       " ('edward', 0): 1,\n",
       " ('behind', 0): 2,\n",
       " ('cricket', 0): 2,\n",
       " ('ground', 0): 1,\n",
       " ('gim', 0): 2,\n",
       " ('ring', 0): 11,\n",
       " ('closebi', 0): 1,\n",
       " ('tuesday', 0): 5,\n",
       " ('order', 0): 8,\n",
       " ('slipper', 0): 2,\n",
       " ('co', 0): 60,\n",
       " ('pay', 0): 35,\n",
       " ('return', 0): 11,\n",
       " ('but', 0): 111,\n",
       " ('arrang', 0): 4,\n",
       " ('dai', 0): 5,\n",
       " ('da', 0): 102,\n",
       " ('can', 0): 71,\n",
       " ('send', 0): 108,\n",
       " ('resum', 0): 2,\n",
       " ('id', 0): 25,\n",
       " ('reali', 0): 5,\n",
       " ('sorryi', 0): 1,\n",
       " ('recognis', 0): 3,\n",
       " ('number', 0): 53,\n",
       " ('confus', 0): 3,\n",
       " ('pleas', 0): 63,\n",
       " ('8007', 1): 19,\n",
       " ('25p', 1): 6,\n",
       " ('alfi', 1): 3,\n",
       " ('moon', 1): 3,\n",
       " ('children', 1): 3,\n",
       " ('need', 1): 8,\n",
       " ('song', 1): 3,\n",
       " ('ur', 1): 117,\n",
       " ('mob', 1): 20,\n",
       " ('tell', 1): 14,\n",
       " ('m8', 1): 4,\n",
       " ('tone', 1): 54,\n",
       " ('chariti', 1): 9,\n",
       " ('nokia', 1): 56,\n",
       " ('poli', 1): 20,\n",
       " ('zed', 1): 6,\n",
       " ('08701417012', 1): 3,\n",
       " ('profit', 1): 3,\n",
       " ('massag', 0): 1,\n",
       " ('use', 0): 44,\n",
       " ('lot', 0): 38,\n",
       " ('oil', 0): 2,\n",
       " ('what', 0): 109,\n",
       " ('fave', 0): 3,\n",
       " ('posit', 0): 4,\n",
       " ('anyth', 0): 64,\n",
       " ('outsid', 0): 15,\n",
       " ('ya', 0): 46,\n",
       " ('vikki', 0): 3,\n",
       " ('vl', 0): 6,\n",
       " ('c', 0): 47,\n",
       " ('witin', 0): 1,\n",
       " ('ltgt', 0): 235,\n",
       " ('min', 0): 29,\n",
       " ('il', 0): 6,\n",
       " ('repli', 0): 40,\n",
       " ('about', 0): 5,\n",
       " ('buck', 0): 5,\n",
       " ('the', 0): 84,\n",
       " ('bank', 0): 14,\n",
       " ('fee', 0): 3,\n",
       " ('fix', 0): 10,\n",
       " ('find', 0): 39,\n",
       " ('greet', 0): 7,\n",
       " ('consid', 0): 5,\n",
       " ('excus', 0): 7,\n",
       " ('yar', 0): 13,\n",
       " ('tot', 0): 17,\n",
       " ('knew', 0): 10,\n",
       " ('di', 0): 27,\n",
       " ('mobil', 0): 15,\n",
       " ('ad', 0): 9,\n",
       " ('contact', 0): 13,\n",
       " ('list', 0): 7,\n",
       " ('wwwfullonsmscom', 0): 2,\n",
       " ('place', 0): 49,\n",
       " ('free', 0): 47,\n",
       " ('sm', 0): 16,\n",
       " ('peopl', 0): 39,\n",
       " ('fullonsmscom', 0): 6,\n",
       " ('sorri', 0): 112,\n",
       " ('grocer', 0): 1,\n",
       " ('final', 1): 15,\n",
       " ('claim', 1): 103,\n",
       " ('å£150', 1): 22,\n",
       " ('worth', 1): 13,\n",
       " ('discount', 1): 9,\n",
       " ('voucher', 1): 30,\n",
       " ('today', 1): 29,\n",
       " ('ye', 1): 14,\n",
       " ('85023', 1): 7,\n",
       " ('savamob', 1): 13,\n",
       " ('member', 1): 8,\n",
       " ('offer', 1): 31,\n",
       " ('mobil', 1): 115,\n",
       " ('t', 1): 13,\n",
       " ('cs', 1): 14,\n",
       " ('pobox84', 1): 6,\n",
       " ('m263uz', 1): 4,\n",
       " ('å£300', 1): 7,\n",
       " ('sub', 1): 9,\n",
       " ('16', 1): 20,\n",
       " ('boy', 0): 32,\n",
       " ('gal', 0): 12,\n",
       " ('he', 0): 71,\n",
       " ('propsd', 0): 2,\n",
       " ('mind', 0): 29,\n",
       " ('gv', 0): 2,\n",
       " ('lv', 0): 2,\n",
       " ('lttr', 0): 2,\n",
       " ('frnd', 0): 18,\n",
       " ('threw', 0): 2,\n",
       " ('thm', 0): 3,\n",
       " ('again', 0): 4,\n",
       " ('decid', 0): 26,\n",
       " ('aproach', 0): 2,\n",
       " ('dt', 0): 2,\n",
       " ('truck', 0): 2,\n",
       " ('speed', 0): 4,\n",
       " ('toward', 0): 7,\n",
       " ('wn', 0): 3,\n",
       " ('hit', 0): 7,\n",
       " ('girld', 0): 2,\n",
       " ('ran', 0): 5,\n",
       " ('hell', 0): 12,\n",
       " ('save', 0): 10,\n",
       " ('she', 0): 60,\n",
       " ('hw', 0): 4,\n",
       " ('cn', 0): 3,\n",
       " ('fast', 0): 11,\n",
       " ('d', 0): 4,\n",
       " ('boost', 0): 5,\n",
       " ('secret', 0): 6,\n",
       " ('energi', 0): 7,\n",
       " ('instantli', 0): 2,\n",
       " ('shout', 0): 3,\n",
       " ('thi', 0): 47,\n",
       " ('live', 0): 20,\n",
       " ('happili', 0): 2,\n",
       " ('2gthr', 0): 2,\n",
       " ('drink', 0): 17,\n",
       " ('evrydi', 0): 2,\n",
       " ('moral', 0): 5,\n",
       " ('hv', 0): 4,\n",
       " ('msgsd', 0): 2,\n",
       " ('gud', 0): 39,\n",
       " ('ni8', 0): 10,\n",
       " ('ta', 0): 13,\n",
       " ('collect', 0): 7,\n",
       " ('car', 0): 34,\n",
       " ('lei', 0): 20,\n",
       " ('cashbal', 1): 6,\n",
       " ('current', 1): 10,\n",
       " ('500', 1): 17,\n",
       " ('pound', 1): 21,\n",
       " ('maxim', 1): 6,\n",
       " ('cashin', 1): 6,\n",
       " ('send', 1): 56,\n",
       " ('collect', 1): 42,\n",
       " ('83600', 1): 4,\n",
       " ('150pmsg', 1): 13,\n",
       " ('cc', 1): 5,\n",
       " ('08718720201', 1): 4,\n",
       " ('po', 1): 24,\n",
       " ('box', 1): 24,\n",
       " ('11414', 1): 3,\n",
       " ('tcrw1', 1): 3,\n",
       " ('typic', 0): 1,\n",
       " ('wat', 0): 84,\n",
       " ('dearer', 0): 3,\n",
       " ('dat', 0): 32,\n",
       " ('meet', 0): 87,\n",
       " ('pain', 0): 23,\n",
       " ('dem', 0): 4,\n",
       " ('mm', 0): 7,\n",
       " ('today', 0): 107,\n",
       " ('na', 0): 81,\n",
       " ('chat', 0): 12,\n",
       " ('drop', 0): 19,\n",
       " ('text', 0): 67,\n",
       " ('your', 0): 78,\n",
       " ('bore', 0): 19,\n",
       " ('etc', 0): 5,\n",
       " ('hope', 0): 92,\n",
       " ('well', 0): 91,\n",
       " ('nose', 0): 1,\n",
       " ('essay', 0): 1,\n",
       " ('xx', 0): 7,\n",
       " ('class', 0): 42,\n",
       " ('rose', 0): 7,\n",
       " ('redr', 0): 2,\n",
       " ('bloodblood', 0): 2,\n",
       " ('heartheart', 0): 2,\n",
       " ('ti', 0): 5,\n",
       " ('includ', 0): 5,\n",
       " ('1u', 0): 3,\n",
       " ('poor', 0): 8,\n",
       " ('relat', 0): 6,\n",
       " ('2u', 0): 4,\n",
       " ('1', 0): 39,\n",
       " ('support', 0): 9,\n",
       " ('3u', 0): 3,\n",
       " ('mani', 0): 37,\n",
       " ('4some1', 0): 2,\n",
       " ('luv', 0): 28,\n",
       " ('5+', 0): 2,\n",
       " ('some1', 0): 5,\n",
       " ('pray', 0): 10,\n",
       " ('marri', 0): 9,\n",
       " ('ye', 0): 55,\n",
       " ('watch', 0): 53,\n",
       " ('footi', 0): 1,\n",
       " ('worri', 0): 30,\n",
       " ('blow', 0): 2,\n",
       " ('phil', 0): 1,\n",
       " ('nevil', 0): 1,\n",
       " ('vijay', 0): 4,\n",
       " ('jaya', 0): 2,\n",
       " ('tv', 0): 18,\n",
       " ('bigger', 0): 3,\n",
       " ('urgent', 1): 56,\n",
       " ('we', 1): 26,\n",
       " ('contact', 1): 54,\n",
       " ('u', 1): 121,\n",
       " ('draw', 1): 30,\n",
       " ('show', 1): 27,\n",
       " ('å£800', 1): 7,\n",
       " ('prize', 1): 84,\n",
       " ('guarante', 1): 44,\n",
       " ('09050003091', 1): 2,\n",
       " ('land', 1): 14,\n",
       " ('line', 1): 31,\n",
       " ('c52', 1): 2,\n",
       " ('valid', 1): 18,\n",
       " ('12hr', 1): 12,\n",
       " ('oh', 0): 87,\n",
       " ('realli', 0): 67,\n",
       " ('air', 0): 5,\n",
       " ('talent', 0): 2,\n",
       " ('rock', 0): 13,\n",
       " ('second', 0): 19,\n",
       " ('hide', 0): 3,\n",
       " ('thousand', 0): 1,\n",
       " ('wonder', 0): 33,\n",
       " ('n8', 0): 1,\n",
       " ('tick', 0): 3,\n",
       " ('where', 0): 44,\n",
       " ('could', 0): 46,\n",
       " ('die', 0): 17,\n",
       " ('loneli', 0): 1,\n",
       " ('pout', 0): 3,\n",
       " ('stomp', 0): 3,\n",
       " ('feet', 0): 4,\n",
       " ('woke', 0): 8,\n",
       " ('up', 0): 19,\n",
       " ('befor', 0): 3,\n",
       " ('fuck', 0): 40,\n",
       " ('3', 0): 37,\n",
       " ('wouldnt', 0): 4,\n",
       " ('be', 0): 12,\n",
       " ('knowyetund', 0): 1,\n",
       " ('hasnt', 0): 4,\n",
       " ('money', 0): 42,\n",
       " ('bother', 0): 7,\n",
       " ('involv', 0): 2,\n",
       " ('shouldnt', 0): 4,\n",
       " ('impos', 0): 1,\n",
       " ('apologis', 0): 3,\n",
       " ('turn', 0): 11,\n",
       " ('show', 0): 28,\n",
       " ('wont', 0): 44,\n",
       " ('til', 0): 20,\n",
       " ('ahead', 0): 7,\n",
       " ('smoke', 0): 20,\n",
       " ('worth', 0): 4,\n",
       " ('callsmessagesmiss', 0): 2,\n",
       " ('outbid', 1): 1,\n",
       " ('simonwatson5120', 1): 1,\n",
       " ('shinco', 1): 1,\n",
       " ('dvd', 1): 4,\n",
       " ('plyr', 1): 1,\n",
       " ('bid', 1): 8,\n",
       " ('visit', 1): 6,\n",
       " ('sm', 1): 18,\n",
       " ('acsmsreward', 1): 1,\n",
       " ('end', 1): 17,\n",
       " ('notif', 1): 1,\n",
       " ('repli', 1): 85,\n",
       " ('out', 1): 8,\n",
       " ('dear', 0): 70,\n",
       " ('yahoo', 0): 5,\n",
       " ('bring', 0): 30,\n",
       " ('perf', 0): 1,\n",
       " ('or', 0): 29,\n",
       " ('legal', 0): 4,\n",
       " ('wot', 0): 19,\n",
       " ('4', 0): 140,\n",
       " ('prob', 0): 14,\n",
       " ('voucher', 0): 1,\n",
       " ('frm', 0): 10,\n",
       " ('virgin', 0): 1,\n",
       " ('sumf', 0): 1,\n",
       " ('trust', 0): 7,\n",
       " ('buy', 0): 59,\n",
       " ('new', 0): 59,\n",
       " ('asap', 0): 2,\n",
       " ('carlosl', 0): 1,\n",
       " ('minut', 0): 34,\n",
       " ('one', 0): 143,\n",
       " ('interest', 0): 13,\n",
       " ('may', 0): 32,\n",
       " ('busi', 0): 14,\n",
       " ('plan', 0): 44,\n",
       " ('search', 0): 14,\n",
       " ('onlin', 0): 17,\n",
       " ('let', 0): 73,\n",
       " ('th', 0): 14,\n",
       " ('gower', 0): 1,\n",
       " ('mate', 0): 13,\n",
       " ('which', 0): 11,\n",
       " ('am', 0): 26,\n",
       " ('all', 0): 34,\n",
       " ('in', 0): 39,\n",
       " ('wale', 0): 1,\n",
       " ('åômorrow', 0): 1,\n",
       " ('wk', 0): 7,\n",
       " ('who', 0): 20,\n",
       " ('åð', 0): 3,\n",
       " ('random', 0): 3,\n",
       " ('liao', 0): 26,\n",
       " ('plz', 0): 21,\n",
       " ('sir', 0): 30,\n",
       " ('world', 0): 30,\n",
       " ('happiest', 0): 1,\n",
       " ('never', 0): 36,\n",
       " ('charact', 0): 5,\n",
       " ('dey', 0): 4,\n",
       " ('best', 0): 28,\n",
       " ('understand', 0): 13,\n",
       " ('differ', 0): 11,\n",
       " ('compani', 0): 13,\n",
       " ('told', 0): 39,\n",
       " ('stupid', 0): 9,\n",
       " ('hear', 0): 21,\n",
       " ('brother', 0): 16,\n",
       " ('spoken', 0): 3,\n",
       " ('not', 0): 50,\n",
       " ('hello', 0): 37,\n",
       " ('boytoy', 0): 13,\n",
       " ('made', 0): 20,\n",
       " ('constant', 0): 2,\n",
       " ('thought', 0): 40,\n",
       " ('wait', 0): 72,\n",
       " ('till', 0): 20,\n",
       " ('hate', 0): 6,\n",
       " ('accept', 0): 9,\n",
       " ('singl', 0): 7,\n",
       " ('mine', 0): 14,\n",
       " ('nicenicehow', 0): 1,\n",
       " ('address', 0): 16,\n",
       " ('applespairsal', 0): 1,\n",
       " ('malarki', 0): 1,\n",
       " ('hict', 0): 1,\n",
       " ('employe', 0): 1,\n",
       " ('might', 0): 28,\n",
       " ('plm', 0): 2,\n",
       " ('on', 0): 25,\n",
       " ('way', 0): 81,\n",
       " ('get', 1): 67,\n",
       " ('free', 1): 171,\n",
       " ('video', 1): 25,\n",
       " ('player', 1): 8,\n",
       " ('movi', 1): 2,\n",
       " ('to', 1): 60,\n",
       " ('go', 1): 32,\n",
       " ('89105', 1): 1,\n",
       " ('it', 1): 7,\n",
       " ('extra', 1): 7,\n",
       " ('film', 1): 3,\n",
       " ('order', 1): 13,\n",
       " ('ts', 1): 3,\n",
       " ('appli', 1): 25,\n",
       " ('18', 1): 19,\n",
       " ('yr', 1): 13,\n",
       " ('with', 0): 17,\n",
       " ('si', 0): 19,\n",
       " ('lor', 0): 137,\n",
       " ('we', 0): 65,\n",
       " ('italian', 0): 4,\n",
       " ('job', 0): 35,\n",
       " ('studi', 0): 18,\n",
       " ('sch', 0): 17,\n",
       " ('cme', 0): 2,\n",
       " ('ho', 0): 5,\n",
       " ('2morow', 0): 2,\n",
       " ('after', 0): 13,\n",
       " ('wil', 0): 15,\n",
       " ('bahama', 1): 4,\n",
       " ('callfreefon', 1): 2,\n",
       " ('08081560665', 1): 2,\n",
       " ('speak', 1): 9,\n",
       " ('live', 1): 28,\n",
       " ('oper', 1): 13,\n",
       " ('either', 1): 7,\n",
       " ('cruis', 1): 2,\n",
       " ('ofå£2000', 1): 2,\n",
       " ('18+onli', 1): 3,\n",
       " ('opt', 1): 11,\n",
       " ('x', 1): 9,\n",
       " ('07786200117', 1): 2,\n",
       " ('right', 0): 74,\n",
       " ('phone', 0): 69,\n",
       " ('tht', 0): 9,\n",
       " ('gift', 0): 9,\n",
       " ('bird', 0): 5,\n",
       " ('human', 0): 2,\n",
       " ('hav', 0): 22,\n",
       " ('natur', 0): 9,\n",
       " ('blackberri', 0): 4,\n",
       " ('torch', 0): 6,\n",
       " ('nigeria', 0): 6,\n",
       " ('buyer', 0): 2,\n",
       " ('melik', 0): 1,\n",
       " ('4a', 0): 2,\n",
       " ('month', 0): 34,\n",
       " ('and', 0): 87,\n",
       " ('bb', 0): 8,\n",
       " ('kidz', 0): 4,\n",
       " ('scream', 0): 10,\n",
       " ('surpris', 0): 9,\n",
       " ('sofa', 0): 7,\n",
       " ('nake', 0): 6,\n",
       " ('probabl', 0): 23,\n",
       " ('winner', 1): 14,\n",
       " ('as', 1): 10,\n",
       " ('valu', 1): 11,\n",
       " ('network', 1): 24,\n",
       " ('custom', 1): 45,\n",
       " ('select', 1): 26,\n",
       " ('receivea', 1): 2,\n",
       " ('å£900', 1): 7,\n",
       " ('reward', 1): 10,\n",
       " ('09061701461', 1): 2,\n",
       " ('code', 1): 24,\n",
       " ('kl341', 1): 2,\n",
       " ('12', 1): 9,\n",
       " ('hour', 1): 5,\n",
       " ('doctor', 0): 10,\n",
       " ('hen', 0): 1,\n",
       " ('night', 0): 92,\n",
       " ('swing', 0): 11,\n",
       " ('g', 0): 13,\n",
       " ('x', 0): 34,\n",
       " ('head', 0): 19,\n",
       " ('cal', 0): 5,\n",
       " ('5', 0): 24,\n",
       " ('nightsw', 0): 1,\n",
       " ('nt', 0): 10,\n",
       " ('port', 0): 1,\n",
       " ('step', 0): 3,\n",
       " ('liaotoo', 0): 1,\n",
       " ('ex', 0): 9,\n",
       " ('kkthi', 0): 1,\n",
       " ('kote', 0): 1,\n",
       " ('birthday', 0): 26,\n",
       " ('there', 0): 32,\n",
       " ('sens', 0): 7,\n",
       " ('foot', 0): 2,\n",
       " ('peni', 0): 2,\n",
       " ('no', 1): 38,\n",
       " ('1', 1): 26,\n",
       " ('everi', 1): 24,\n",
       " ('week', 1): 47,\n",
       " ('just', 1): 22,\n",
       " ('nok', 1): 1,\n",
       " ('87021', 1): 2,\n",
       " ('1st', 1): 17,\n",
       " ('txtin', 1): 2,\n",
       " ('friend', 1): 13,\n",
       " ('150ptone', 1): 3,\n",
       " ('hl', 1): 4,\n",
       " ('4info', 1): 2,\n",
       " ('camera', 1): 30,\n",
       " ('you', 1): 85,\n",
       " ('award', 1): 48,\n",
       " ('sipix', 1): 5,\n",
       " ('digit', 1): 7,\n",
       " ('09061221066', 1): 3,\n",
       " ('fromm', 1): 3,\n",
       " ('landlin', 1): 23,\n",
       " ('deliveri', 1): 18,\n",
       " ('within', 1): 6,\n",
       " ('28', 1): 4,\n",
       " ('day', 1): 23,\n",
       " ('an', 0): 11,\n",
       " ('bslvyl', 0): 11,\n",
       " ('via', 0): 8,\n",
       " ('dayu', 0): 3,\n",
       " ('sister', 0): 17,\n",
       " ('lover', 0): 5,\n",
       " ('dear1', 0): 3,\n",
       " ('best1', 0): 3,\n",
       " ('clos1', 0): 3,\n",
       " ('lvblefrnd', 0): 3,\n",
       " ('jstfrnd', 0): 3,\n",
       " ('cutefrnd', 0): 3,\n",
       " ('lifpartnr', 0): 3,\n",
       " ('belovd', 0): 4,\n",
       " ('swtheart', 0): 3,\n",
       " ('bstfrnd', 0): 3,\n",
       " ('rpli', 0): 4,\n",
       " ('mean', 0): 43,\n",
       " ('enemi', 0): 3,\n",
       " ('sec', 0): 3,\n",
       " ('new', 1): 58,\n",
       " ('textbuddi', 1): 1,\n",
       " ('horni', 1): 5,\n",
       " ('guy', 1): 4,\n",
       " ('area', 1): 7,\n",
       " ('receiv', 1): 30,\n",
       " ('search', 1): 1,\n",
       " ('postcod', 1): 2,\n",
       " ('gaytextbuddycom', 1): 1,\n",
       " ('one', 1): 7,\n",
       " ('name', 1): 12,\n",
       " ('89693', 1): 1,\n",
       " ('08715500022', 1): 1,\n",
       " ('rpl', 1): 1,\n",
       " ('cnl', 1): 1,\n",
       " ('avail', 0): 12,\n",
       " ('hillsborough', 0): 1,\n",
       " ('cook', 0): 5,\n",
       " ('rather', 0): 6,\n",
       " ('salmon', 0): 1,\n",
       " ('la', 0): 6,\n",
       " ('quiet', 0): 2,\n",
       " ('beth', 0): 1,\n",
       " ('aunt', 0): 4,\n",
       " ('charli', 0): 1,\n",
       " ('helen', 0): 2,\n",
       " ('mo', 0): 7,\n",
       " ('scratch', 0): 2,\n",
       " ('isnt', 0): 15,\n",
       " ('neces', 0): 1,\n",
       " ('imagin', 0): 4,\n",
       " ('urself', 0): 9,\n",
       " ('witout', 0): 1,\n",
       " ('hwd', 0): 1,\n",
       " ('colleg', 0): 13,\n",
       " ('watll', 0): 1,\n",
       " ('wth', 0): 1,\n",
       " ('cell', 0): 5,\n",
       " ('abt', 0): 16,\n",
       " ('function', 0): 2,\n",
       " ('thnk', 0): 7,\n",
       " ('event', 0): 3,\n",
       " ('espel', 0): 1,\n",
       " ('care', 0): 59,\n",
       " ('irrit', 0): 7,\n",
       " ('4wrd', 0): 1,\n",
       " ('dearlov', 0): 1,\n",
       " ('wthout', 0): 1,\n",
       " ('jst', 0): 2,\n",
       " ('takecar', 0): 1,\n",
       " ('goodmorn', 0): 11,\n",
       " ('for', 1): 30,\n",
       " ('sale', 1): 2,\n",
       " ('arsen', 1): 1,\n",
       " ('dartboard', 1): 1,\n",
       " ('good', 1): 12,\n",
       " ('condit', 1): 4,\n",
       " ('doubl', 1): 15,\n",
       " ('trebl', 1): 1,\n",
       " ('breez', 0): 1,\n",
       " ('bright', 0): 3,\n",
       " ('sun', 0): 14,\n",
       " ('fresh', 0): 1,\n",
       " ('flower', 0): 3,\n",
       " ('twitter', 0): 1,\n",
       " ('loti', 0): 1,\n",
       " ('bought', 0): 2,\n",
       " ('egg', 0): 5,\n",
       " ('fanci', 0): 5,\n",
       " ('someth', 0): 59,\n",
       " ('gotten', 0): 2,\n",
       " ('begun', 0): 1,\n",
       " ('registr', 0): 1,\n",
       " ('perman', 0): 1,\n",
       " ('resid', 0): 1,\n",
       " ('ever', 0): 27,\n",
       " ('word', 0): 25,\n",
       " ('sure', 0): 62,\n",
       " ('comei', 0): 1,\n",
       " ('touch', 0): 15,\n",
       " ('yeah', 0): 64,\n",
       " ('quit', 0): 32,\n",
       " ('bit', 0): 39,\n",
       " ('left', 0): 24,\n",
       " ('tomorrow', 0): 67,\n",
       " ('congrat', 0): 5,\n",
       " ('2u2', 0): 1,\n",
       " ('mu', 0): 12,\n",
       " ('figur', 0): 6,\n",
       " ('everyon', 0): 10,\n",
       " ('ga', 0): 11,\n",
       " ('alcohol', 0): 2,\n",
       " ('jay', 0): 9,\n",
       " ('weed', 0): 5,\n",
       " ('budget', 0): 6,\n",
       " ('xavier', 0): 2,\n",
       " ('aiyah', 0): 4,\n",
       " ('improv', 0): 1,\n",
       " ('speak', 0): 20,\n",
       " ('they', 0): 21,\n",
       " ('treat', 0): 15,\n",
       " ('aid', 0): 2,\n",
       " ('patent', 0): 1,\n",
       " ('glad', 0): 9,\n",
       " ('darren', 0): 10,\n",
       " ('oso', 0): 20,\n",
       " ('sian', 0): 1,\n",
       " ('tmr', 0): 21,\n",
       " ('haf', 0): 17,\n",
       " ('lect', 0): 7,\n",
       " ('now', 0): 27,\n",
       " ('dumb', 0): 1,\n",
       " ('gener', 0): 2,\n",
       " ('price', 0): 5,\n",
       " ('oz', 0): 2,\n",
       " ('ifwhenhow', 0): 1,\n",
       " ('when', 0): 41,\n",
       " ('roommat', 0): 6,\n",
       " ('dun', 0): 45,\n",
       " ('thk', 0): 44,\n",
       " ('hmmm', 0): 4,\n",
       " ('jazz', 0): 5,\n",
       " ('yogasana', 0): 5,\n",
       " ('em', 0): 8,\n",
       " ('lesson', 0): 21,\n",
       " ('den', 0): 29,\n",
       " ('late', 0): 44,\n",
       " ('2hr', 0): 1,\n",
       " ('becaus', 0): 5,\n",
       " ('lol', 0): 55,\n",
       " ('real', 0): 18,\n",
       " ('cancer', 0): 5,\n",
       " ('depend', 0): 7,\n",
       " ('qualiti', 0): 4,\n",
       " ('type', 0): 13,\n",
       " ('fade', 0): 1,\n",
       " ('glori', 0): 1,\n",
       " ('ralph', 0): 1,\n",
       " ('uawakefeellikw', 0): 1,\n",
       " ('shitjustfound', 0): 1,\n",
       " ('out', 0): 5,\n",
       " ('alett', 0): 1,\n",
       " ('thatmum', 0): 1,\n",
       " ('gotmarri', 0): 1,\n",
       " ('4thnovbehind', 0): 1,\n",
       " ('ourback', 0): 1,\n",
       " ('fuckinniceselfish', 0): 1,\n",
       " ('noth', 0): 25,\n",
       " ('everybodi', 0): 5,\n",
       " ('congrat', 1): 8,\n",
       " ('year', 1): 11,\n",
       " ('special', 1): 17,\n",
       " ('cinema', 1): 3,\n",
       " ('pass', 1): 5,\n",
       " ...}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ccb69e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_naive_bayes(freqs, train_x, train_y):\n",
    "    '''\n",
    "    Input:\n",
    "        freqs: dictionary from (word, label) to how often the word appears\n",
    "        train_x: a list of tweets\n",
    "        train_y: a list of labels correponding to the tweets (0,1)\n",
    "    Output:\n",
    "        logprior: the log prior. (equation 3 above)\n",
    "        loglikelihood: the log likelihood of you Naive bayes equation. (equation 6 above)\n",
    "    '''\n",
    "    loglikelihood = {}\n",
    "    logprior = 0\n",
    "\n",
    "    \n",
    "\n",
    "    # calculate V, the number of unique words in the vocabulary\n",
    "    vocab = set([pair[0] for pair in freqs.keys()])\n",
    "    V = len(vocab)\n",
    "\n",
    "    # calculate num_pos and num_neg\n",
    "    num_pos = num_neg = 0\n",
    "    for pair in freqs.keys():\n",
    "        # if the label is positive (greater than zero)\n",
    "        if pair[1] > 0:\n",
    "\n",
    "            # Increment the number of positive words by the count for this (word, label) pair\n",
    "            num_pos += freqs[pair]\n",
    "\n",
    "        # else, the label is negative\n",
    "        else:\n",
    "\n",
    "            # increment the number of negative words by the count for this (word,label) pair\n",
    "            num_neg += freqs[pair]\n",
    "\n",
    "    # Calculate D, the number of documents\n",
    "    num_doc = len(train_y)\n",
    "\n",
    "    # Calculate pos_num_docs, the number of positive documents (*hint: use sum(<np_array>))\n",
    "    pos_num_docs = (len(list(filter(lambda x: x > 0, train_y))))\n",
    "\n",
    "    # Calculate neg_num_docs, the number of negative documents (*hint: compute using D and pos_num_docs)\n",
    "    neg_num_docs = (len(list(filter(lambda x: x <= 0, train_y))))\n",
    "\n",
    "    # Calculate logprior\n",
    "    logprior = np.log(pos_num_docs) - np.log(neg_num_docs)\n",
    "\n",
    "    # For each word in the vocabulary...\n",
    "    for word in vocab:\n",
    "        # get the positive and negative frequency of the word\n",
    "        freq_pos = find(freqs,word,1)\n",
    "        freq_neg = find(freqs,word,0)\n",
    "\n",
    "        # calculate the probability that each word is positive, and negative\n",
    "        p_w_pos = (freq_pos + 1) / (num_pos + V)\n",
    "        p_w_neg = (freq_neg + 1) / (num_neg + V)\n",
    "\n",
    "        # calculate the log likelihood of the word\n",
    "        loglikelihood[word] = np.log(p_w_pos/p_w_neg)\n",
    "\n",
    "\n",
    "\n",
    "    return logprior, loglikelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "8381e8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.8434905440756992\n",
      "7262\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n",
    "logprior, loglikelihood = train_naive_bayes(freqs, X_train, y_train)\n",
    "print(logprior)\n",
    "print(len(loglikelihood))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ffec2b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 4 CELL\n",
    "\n",
    "def naive_bayes_predict(tweet, logprior, loglikelihood):\n",
    "    '''\n",
    "    Params:\n",
    "        tweet: a string\n",
    "        logprior: a number\n",
    "        loglikelihood: a dictionary of words mapping to numbers\n",
    "    Return:\n",
    "        total_prob: the sum of all the logliklihoods of each word in the tweet (if found in the dictionary) + logprior (a number)\n",
    "\n",
    "    '''\n",
    "    \n",
    "    # process the tweet to get a list of words\n",
    "    word_l = clean_tweet(tweet)\n",
    "\n",
    "    # initialize probability to zero\n",
    "    total_prob = 0\n",
    "\n",
    "    # add the logprior\n",
    "    total_prob += logprior\n",
    "\n",
    "    for word in word_l:\n",
    "\n",
    "        # check if the word exists in the loglikelihood dictionary\n",
    "        if word in loglikelihood:\n",
    "            # add the log likelihood of that word to the probability\n",
    "            total_prob += loglikelihood[word]\n",
    "\n",
    "\n",
    "\n",
    "    return total_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "927c3c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The expected output is -8.139492247399499\n"
     ]
    }
   ],
   "source": [
    "# You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n",
    "\n",
    "# Experiment with your own tweet.\n",
    "my_tweet = 'She smiled.'\n",
    "p = naive_bayes_predict(my_tweet, logprior, loglikelihood)\n",
    "print('The expected output is', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "873cf60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "def test_naive_bayes(test_x, test_y, logprior, loglikelihood):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        test_x: A list of tweets\n",
    "        test_y: the corresponding labels for the list of tweets\n",
    "        logprior: the logprior\n",
    "        loglikelihood: a dictionary with the loglikelihoods for each word\n",
    "    Output:\n",
    "        accuracy: (# of tweets classified correctly)/(total # of tweets)\n",
    "    \"\"\"\n",
    "    accuracy = 0  # return this properly\n",
    "\n",
    "    \n",
    "    y_hats = []\n",
    "    for tweet in test_x:\n",
    "        # if the prediction is > 0\n",
    "        if naive_bayes_predict(tweet, logprior, loglikelihood) > 0:\n",
    "            # the predicted class is 1\n",
    "            y_hat_i = 1\n",
    "        else:\n",
    "            # otherwise the predicted class is 0\n",
    "            y_hat_i = 0\n",
    "\n",
    "        # append the predicted class to the list y_hats\n",
    "        y_hats.append(y_hat_i)\n",
    "\n",
    "    # error is the average of the absolute values of the differences between y_hats and test_y\n",
    "    error = np.mean(np.absolute(y_hats-test_y))\n",
    "\n",
    "    # Accuracy is 1 minus the error\n",
    "    accuracy = 1-error\n",
    "\n",
    "\n",
    "\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "419dfc63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am happy -> -7.59\n",
      "I am bad -> -6.28\n",
      "this movie should have been great. -> -4.07\n",
      "great -> -2.95\n",
      "great great -> -4.07\n",
      "great great great -> -5.18\n",
      "great great great great -> -6.29\n"
     ]
    }
   ],
   "source": [
    "# For grading purpose only\n",
    "# You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n",
    "\n",
    "# Run this cell to test your function\n",
    "for tweet in ['I am happy', 'I am bad', 'this movie should have been great.', 'great', 'great great', 'great great great', 'great great great great']:\n",
    "    # print( '%s -> %f' % (tweet, naive_bayes_predict(tweet, logprior, loglikelihood)))\n",
    "    p = naive_bayes_predict(tweet, logprior, loglikelihood)\n",
    "#     print(f'{tweet} -> {p:.2f} ({p_category})')\n",
    "    print(f'{tweet} -> {p:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "4781746d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.535091004750317"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feel free to check the sentiment of your own tweet below\n",
    "my_tweet = 'you are bad :('\n",
    "naive_bayes_predict(my_tweet, logprior, loglikelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d7761b",
   "metadata": {},
   "source": [
    "## Theory Questions: (15 points)\n",
    "\n",
    "1. When performing Naive Bayes operation especially for text classification, why is there a requirement for Laplace Smoothing or Additive Smoothing? Explain with considering an example of training and the test set and show how not having additive smoothing leads to undesirable outcomes. (10 points)\n",
    "\n",
    "\n",
    "2. Why are logarithmic values computed for naive bayes algorithm rather than only the probability values? (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e6cdba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
